{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2abc043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.0.198-py3-none-any.whl (1.0 MB)\n",
      "     ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.1/1.0 MB 991.0 kB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 0.3/1.0 MB 3.2 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.6/1.0 MB 4.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 0.9/1.0 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.0/1.0 MB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from langchain) (6.0)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Downloading SQLAlchemy-2.0.16-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/2.0 MB 10.6 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.7/2.0 MB 9.2 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.0/2.0 MB 8.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 1.4/2.0 MB 9.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 1.8/2.0 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.0/2.0 MB 9.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
      "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
      "Collecting langchainplus-sdk>=0.0.7\n",
      "  Downloading langchainplus_sdk-0.0.8-py3-none-any.whl (22 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Downloading numexpr-2.8.4-cp311-cp311-win_amd64.whl (92 kB)\n",
      "     ---------------------------------------- 0.0/92.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 92.7/92.7 kB 5.2 MB/s eta 0:00:00\n",
      "Collecting numpy<2,>=1\n",
      "  Downloading numpy-1.24.3-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "     ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.4/14.8 MB 12.9 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.8/14.8 MB 10.4 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 1.2/14.8 MB 11.4 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.7/14.8 MB 11.8 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 2.1/14.8 MB 12.1 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.5/14.8 MB 11.4 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 2.9/14.8 MB 11.7 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 3.3/14.8 MB 11.8 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 3.7/14.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ----------- ---------------------------- 4.1/14.8 MB 11.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.5/14.8 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 4.9/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 5.4/14.8 MB 11.4 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.8/14.8 MB 11.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 6.2/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 6.6/14.8 MB 11.8 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 7.0/14.8 MB 11.6 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 7.5/14.8 MB 11.6 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 7.9/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 8.3/14.8 MB 11.8 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 8.7/14.8 MB 11.6 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 9.1/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 9.6/14.8 MB 11.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 10.0/14.8 MB 11.8 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 10.4/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 10.8/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 11.3/14.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 11.7/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 12.1/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 12.5/14.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 12.9/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 13.4/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 13.8/14.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 14.2/14.8 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.6/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.8/14.8 MB 11.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.8/14.8 MB 11.5 MB/s eta 0:00:00\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 0.0/90.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 90.0/90.0 kB ? eta 0:00:00\n",
      "Collecting pydantic<2,>=1\n",
      "  Downloading pydantic-1.10.9-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.4/2.1 MB 12.9 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.8/2.1 MB 13.4 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.3/2.1 MB 11.5 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.7/2.1 MB 11.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.1/2.1 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 11.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Collecting marshmallow<4.0.0,>=3.3.0\n",
      "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 0.0/49.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.1/49.1 kB ? eta 0:00:00\n",
      "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
      "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\g_san\\miniforge3\\envs\\chatgpt_prompt\\lib\\site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp311-cp311-win_amd64.whl (192 kB)\n",
      "     ---------------------------------------- 0.0/192.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 192.5/192.5 kB 11.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\g_san\\appdata\\roaming\\python\\python311\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, pydantic, numpy, mypy-extensions, marshmallow, greenlet, typing-inspect, SQLAlchemy, openapi-schema-pydantic, numexpr, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
      "Successfully installed SQLAlchemy-2.0.16 dataclasses-json-0.5.8 greenlet-2.0.2 langchain-0.0.198 langchainplus-sdk-0.0.8 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 numexpr-2.8.4 numpy-1.24.3 openapi-schema-pydantic-1.2.4 pydantic-1.10.9 tenacity-8.2.2 typing-inspect-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install python-dotenv\n",
    "#!pip install openai\n",
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c408d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "#print(os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3806571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    #print(response)\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d34b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"As an AI language model, I can tell you that the answer to 1+1 is 2.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1686575223,\n",
      "  \"id\": \"chatcmpl-7QbY7ij3goHVvUXyEUWEkLPZCJBr3\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 22,\n",
      "    \"prompt_tokens\": 15,\n",
      "    \"total_tokens\": 37\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I can tell you that the answer to 1+1 is 2.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f7a592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3162401c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(lc_kwargs={'temperature': 0.0}, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21932056",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a0efc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a propt tempplate for ChatPromptTemplate inside the prompt_template variable\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3065da07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lc_kwargs={'input_variables': ['style', 'text'], 'template': 'Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'} input_variables=['style', 'text'] output_parser=None partial_variables={} template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n' template_format='f-string' validate_template=True\n",
      "#######\n",
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "# let's see what there is in the prompt and in the input variables\n",
    "print(prompt_template.messages[0].prompt)\n",
    "print(\"#######\")\n",
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb3c1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the customer style\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "090f5c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0bd60e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d491004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain.schema.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "235f5465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"Hello GP, how can I assist you today?\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1686578564,\n",
      "  \"id\": \"chatcmpl-7QcQ0BgAgF0G9cfvVITEolRhG6Foq\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 10,\n",
      "    \"prompt_tokens\": 12,\n",
      "    \"total_tokens\": 22\n",
      "  }\n",
      "}\n",
      "Hello GP, how can I assist you today?\n",
      "######\n",
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"I'm sorry, as an AI language model, I don't have access to personal information such as your name.\",\n",
      "        \"role\": \"assistant\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1686578566,\n",
      "  \"id\": \"chatcmpl-7QcQ2yDwbzXcGQT3QOW8rqpZtpmog\",\n",
      "  \"model\": \"gpt-3.5-turbo-0301\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 23,\n",
      "    \"prompt_tokens\": 13,\n",
      "    \"total_tokens\": 36\n",
      "  }\n",
      "}\n",
      "I'm sorry, as an AI language model, I don't have access to personal information such as your name.\n"
     ]
    }
   ],
   "source": [
    "# LLMs do not have memory from a request to another:\n",
    "response = get_completion(\"my name is GP\")\n",
    "print(response)\n",
    "\n",
    "print(\"######\")\n",
    "response = get_completion(\"What's my name?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1f307e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use LangChain so that then we can talk to the LLM like in a BOT conversation\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dc3f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50230cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello GP, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is GP, as you mentioned earlier.\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello again, GP! It's nice to see you. Is there anything else I can help you with?\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello GP, it seems like you've already introduced yourself a few times. Is there something else you'd like to talk about?\n",
      "Human: What is 1+1?\n",
      "AI: As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you'd like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is GP, as you mentioned earlier. Is there anything else I can help you with?\n",
      "Human: What is 1+1?\n",
      "AI: As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you'd like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is GP, as you mentioned earlier. Is there anything else I can help you with?\n",
      "Human: Hi, my name is GP\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello GP, it's nice to see you again. Is there something specific you'd like to discuss or ask me about?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is GP\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52d724f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello GP, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is GP, as you mentioned earlier.\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello again, GP! It's nice to see you. Is there anything else I can help you with?\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello GP, it seems like you've already introduced yourself a few times. Is there something else you'd like to talk about?\n",
      "Human: What is 1+1?\n",
      "AI: As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you'd like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is GP, as you mentioned earlier. Is there anything else I can help you with?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you'd like to know?\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2f7af30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in on_chain_start callback: 'name'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello GP, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: What is my name?\n",
      "AI: Your name is GP, as you mentioned earlier.\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello again, GP! It's nice to see you. Is there anything else I can help you with?\n",
      "Human: Hi, my name is GP\n",
      "AI: Hello GP, it seems like you've already introduced yourself a few times. Is there something else you'd like to talk about?\n",
      "Human: What is 1+1?\n",
      "AI: As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you'd like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is GP, as you mentioned earlier. Is there anything else I can help you with?\n",
      "Human: What is 1+1?\n",
      "AI: As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you'd like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is GP, as you mentioned earlier. Is there anything else I can help you with?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e98b2",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13ad2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({})\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b174943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65ec8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fc3ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb633213",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39e8f09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "555323d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchan offers different ways of storing memory\n",
    "# previously we used ConversationBufferMemory\n",
    "# other thypes are\n",
    "# - ConversationBufferWindowMemory\n",
    "# - ConversationTokenBufferMemory\n",
    "# - ConversationSummaryMemory\n",
    "# More on the documentation https://python.langchain.com/en/latest/modules/memory/how_to_guides.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dddb17",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
